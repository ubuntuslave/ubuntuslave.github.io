<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Vision 2π</title>
    <link>https://ubuntuslave.github.io/tags/computer-vision/</link>
      <atom:link href="https://ubuntuslave.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Carlos Jaramillo</copyright><lastBuildDate>Tue, 01 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ubuntuslave.github.io/img/icon.png</url>
      <title>Computer Vision</title>
      <link>https://ubuntuslave.github.io/tags/computer-vision/</link>
    </image>
    
    <item>
      <title>Visual Odometry with a Single-Camera Stereo Omnidirectional System</title>
      <link>https://ubuntuslave.github.io/project/vo_sos/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/project/vo_sos/</guid>
      <description>

&lt;h2 id=&#34;demonstration-video-using-sequences-from-grand-central-terminal-gct&#34;&gt;Demonstration video using sequences from Grand Central Terminal (GCT):&lt;/h2&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/c5tyHqEkKQA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;The demonstration code can be found at the git repository for &lt;a href=&#34;https://github.com/ubuntuslave/vo_single_camera_sos&#34; target=&#34;_blank&#34;&gt;vo_single_camera_sos&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;calibration-files&#34;&gt;Calibration Files&lt;/h2&gt;

&lt;!-- FUTURE:
All the calibration images and &#34;pickles&#34; can be found here:

- [real](ftp://robot:robot@134.74.16.75/datasets/calibration/real)

- [synthetic](ftp://robot:robot@134.74.16.75/datasets/calibration/synthetic)
 
--&gt;

&lt;h2 id=&#34;data-sets-a-name-datasets-a&#34;&gt;Data sets&lt;a name=&#34;datasets&#34;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Each sequence has 2 sub-folders:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;omni&lt;/code&gt; pertaining the omnistereo data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rgbd&lt;/code&gt; pertaining the RGB-D camera data
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;remarks-about-the-ground-truth-data-used-as-reference&#34;&gt;Remarks about the &amp;ldquo;ground-truth&amp;rdquo; data used as reference&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Ground-truth poses obey the &lt;a href=&#34;http://vision.in.tum.de/data/datasets/rgbd-dataset/file_formats&#34; target=&#34;_blank&#34;&gt;TUM format&lt;/a&gt;, such that each line is &lt;em&gt;spaced-separated&lt;/em&gt; encoding:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;time stamp  $t_x$  $t_y$  $t_z$  $q_x$  $q_y$  $q_z$  $q_w$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each sequence, the &lt;code&gt;gt_TUM.txt&lt;/code&gt; is the &lt;em&gt;raw&lt;/em&gt; ground-truth data, which was obtained from the motion capture system. Thus, these poses are given wrt our VICON mocap&amp;rsquo;s frame, $[\rm{V}]$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;After running the &lt;code&gt;demo_vo_*.py&lt;/code&gt; for some sequence, the resulting files will be given inside the &lt;code&gt;results&lt;/code&gt; subfolder within the sequence path:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;estimated_frame_poses_TUM.txt&lt;/code&gt; has the estimated poses ${}_{[{{\rm{C}}_i}]}^{[{\rm{K}_0}]}{\bf{\tilde T}}$ of the sequence wrt the initial camera frame, $[\rm{K}_0]$.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gt_associated_frame_poses_TUM.txt&lt;/code&gt; has the associated ground-truth poses for the registered frames. They are already transformed into the camera frame, $[\textbf{C}]$, via the appropriate &lt;em&gt;hand-eye transformation&lt;/em&gt;, so that the pose is given as ${}_{[{{\rm{C}}_i}]}^{[{\rm{K}_0}]}{\bf{T}}$.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For the &lt;strong&gt;real-life&lt;/strong&gt; sequences of the RGB-D camera, the required &lt;em&gt;hand-eye transformation&lt;/em&gt; can be downloaded from this link &lt;a href=&#34;https://www.dropbox.com/s/0322px08wdyirz1/rgbd_hand_eye_transformation.txt?dl=0&#34; target=&#34;_blank&#34;&gt;rgbd_hand_eye_transformation.txt&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;synthetic-sequences&#34;&gt;Synthetic sequences&lt;/h3&gt;

&lt;p&gt;To run the &lt;code&gt;demo_vo_sos.py&lt;/code&gt; script with the &lt;em&gt;synthetic&lt;/em&gt; data set, it suffices to obtain the corresponding GUMS calibration file &lt;a href=&#34;https://www.dropbox.com/s/9k8s1ah1qmgwqrc/gums-calibrated.pkl?dl=0&#34; target=&#34;_blank&#34;&gt;gums-calibrated.pkl&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th &gt;Name&lt;/th&gt;
    &lt;th &gt;&lt;div align=&#34;center&#34;&gt;# Frames&lt;/div&gt;&lt;/th&gt;
    &lt;th &gt;Video Sample&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/rx3n7i5n3fpsc5k/office-0.zip?dl=0&#34; download&gt;Office-0&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;div align=&#34;center&#34;&gt;1508&lt;/div&gt;&lt;/td&gt;
    &lt;td rowspan=&#34;4&#34;&gt;
    &lt;iframe width=&#34;280&#34; src=&#34;https://www.youtube.com/embed/JXaaB9Qeoas?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/ve3kkq4uoha1b46/office-1.zip?dl=0&#34; download&gt;Office-1&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;div align=&#34;center&#34;&gt;965&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/ore4ukmxh5j0338/office-2.zip?dl=0&#34; download&gt;Office-2&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;div align=&#34;center&#34;&gt;880&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/gak8cwyhookzasu/office-3.zip?dl=0&#34; download&gt;Office-3&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;div align=&#34;center&#34;&gt;1240&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&#34;real-life-sequences&#34;&gt;Real-life sequences&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To run the &lt;code&gt;demo_vo_sos.py&lt;/code&gt; script with the &lt;em&gt;real-life&lt;/em&gt; SOS data set, it suffices to obtain the corresponding GUMS calibration file &lt;a href=&#34;https://www.dropbox.com/s/i01jl165c0mzb8n/gums-calibrated.pkl?dl=0&#34; target=&#34;_blank&#34;&gt;gums-calibrated.pkl&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To run the &lt;code&gt;demo_vo_rgbd.py&lt;/code&gt; script with the &lt;em&gt;real-life&lt;/em&gt; RGB-D data set, the required &lt;em&gt;hand-eye transformation&lt;/em&gt; &lt;a href=&#34;https://www.dropbox.com/s/0322px08wdyirz1/rgbd_hand_eye_transformation.txt?dl=0&#34; target=&#34;_blank&#34;&gt;rgbd_hand_eye_transformation.txt&lt;/a&gt; is needed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;conventional-motion&#34;&gt;Conventional motion&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;# Frames&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/xkxp880mxcvcj8x/square_small.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Square Small&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;619&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/7xt2scl62ofwnxm/square_smooth.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Square Smooth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1325&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/nazxaamjnr6amah/spinning.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Spinning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;770&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/e3xsn9fq20fwwbr/vertical.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Vertical&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/ojggx0nmzg6jh9d/free_style.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Free Style&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;611&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/2kttiewtv20x880/hallway.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Hallway&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;5636&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;moving-under-special-conditions&#34;&gt;Moving under special conditions&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;# Frames&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/9rodx1ma02atnwu/into_wall_regular.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Wall - Regular&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1041&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/dh1c73tsswyqzew/into_wall_slow.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Wall - Slow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1400&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/zymh8znjoxaf2z6/into_wall_fast.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Wall - Fast&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;896&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/mui9v6j1rhzncl6/into_wall_curvy.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Wall - Curvy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;838&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/xnj7e01vm2jfbkx/into_darkness_straight.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Dark - Straight&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;998&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/bw5slevqzrwupk0/into_darkness_turning.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Into Dark - Turning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1260&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;moving-in-dynamic-environments&#34;&gt;Moving in dynamic environments&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;# Frames&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/m3q3xw747eajvtq/moving_in_slow_dynamic_environment.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Slow Dynamic&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;390&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/j2p4mi8sg9h5gng/moving_in_fast_dynamic_environment.zip?dl=0&#34; target=&#34;_blank&#34;&gt;Fast Dynamic&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;518&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/ubbq4q296l669yc/GCT_clock.zip?dl=0&#34; target=&#34;_blank&#34;&gt;GCT Clock&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2179&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/16qhrqzlc4wwzvc/GCT_stairs.zip?dl=0&#34; target=&#34;_blank&#34;&gt;GCT Stairs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;3625&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;static-rigs-in-dynamic-environments&#34;&gt;Static rigs in dynamic environments&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Prox. [m]&lt;/th&gt;
&lt;th&gt;# People&lt;/th&gt;
&lt;th&gt;File Link&lt;/th&gt;
&lt;th&gt;# Frames&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/swt1az2i9rvs9ok/static_dynamic_1_1.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_1_1.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;691&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/chh2oh21ufg1a26/static_dynamic_1_2.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_1_2.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;759&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/6bco2rtpncqrdsl/static_dynamic_1_4.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_1_4.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;791&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/m2u0qx1nsjz3v07/static_dynamic_2_1.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_2_1.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;679&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/b0viyankjrh81tq/static_dynamic_2_2.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_2_2.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;673&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/yw0y63tslvu2zff/static_dynamic_2_4.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_2_4.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;799&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/pkguohcliqd120p/static_dynamic_3_1.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_3_1.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;720&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/d2d062sl8vo1wfp/static_dynamic_3_2.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_3_2.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;815&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/cnr0antcvt5tv3a/static_dynamic_3_4.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_3_4.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;772&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Var&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/kamzqbkgpoey0p3/static_dynamic_freestyle.zip?dl=0&#34; target=&#34;_blank&#34;&gt;static_dynamic_freestyle.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;939&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Var&lt;/td&gt;
&lt;td&gt;Var&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/s/8666co2qu4a61en/GCT_static.zip?dl=0&#34; target=&#34;_blank&#34;&gt;GCT_static.zip&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1904&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;

&lt;p&gt;When using this dataset in your research, please cite:&lt;/p&gt;

&lt;div&gt;&lt;p&gt;
@ARTICLE{&lt;a href=&#34;http://www.to_do.pdf&#34;&gt;Jaramillo2019MVAP&lt;/a&gt;,&lt;br/&gt;
&amp;nbsp; author = {&lt;a href=&#34;http://me.vision2pi.com&#34; target=&#34;blank&#34;&gt;Carlos Jaramillo&lt;/a&gt; 
and 
&lt;a href=&#34;https://ericlyang.github.io&#34; target=&#34;blank&#34;&gt;Liang Yang&lt;/a&gt; 
and 
Pablo Munoz
and 
&lt;a href=&#34;http://yuichitaguchi.com&#34; target=&#34;blank&#34;&gt;Yuichi Taguchi&lt;/a&gt;
and &lt;a href=&#34;http://robotics.ccny.cuny.edu/&#34; target=&#34;blank&#34;&gt;Jizhong Xiao&lt;/a&gt;
}, &lt;br/&gt; &amp;nbsp; 
title = {Visual Odometry with a Single-Camera Stereo Omnidirectional System},&lt;br/&gt;&amp;nbsp; 
journal = {Springer Machine Vision and Applications (MVAP)},&lt;br/&gt; 
&amp;nbsp; year = {2019}&lt;br/&gt;
}
&lt;br/&gt;
&lt;/p&gt;&lt;/div&gt;

&lt;h2 id=&#34;copyright&#34;&gt;Copyright&lt;/h2&gt;

&lt;div&gt;&lt;p&gt;
&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0;float:left;margin-right:10px;&#34; src=&#34;http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png&#34; /&gt;All datasets on this page are copyrighted by Carlos Jaramillo and published under the &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/3.0/&#34;&gt;Creative Commons Attribution-NonCommercial-ShareAlike 3.0&lt;/a&gt; License. 
&lt;br/&gt;
This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.&lt;br&gt;
&lt;/p&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visual Odometry with a Single-Camera Stereo Omnidirectional System</title>
      <link>https://ubuntuslave.github.io/publication/2019-mvap-sos_vo/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2019-mvap-sos_vo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhancing 3D Visual Odometry with Single-Camera Stereo Omnidirectional Systems</title>
      <link>https://ubuntuslave.github.io/publication/2018-phd_thesis/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2018-phd_thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Direct Multichannel Tracking</title>
      <link>https://ubuntuslave.github.io/publication/2017-3dv-dmt/</link>
      <pubDate>Thu, 25 May 2017 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2017-3dv-dmt/</guid>
      <description>

&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;

&lt;p&gt;The following figure explains the pipeline of the DMT system:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;DMT-pipeline-white_bg.png&#34; alt=&#34;DMT System Pipeline&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The single channel energy equation for photometric alignment is
$$
E(\xi ) = \sum\limits_{i = 1}^V {{{({{I_K}({{\bf{p}}_i}) - I(\omega ({{\bf{p}}_i},{{\bf{D}}_K}({{\bf{p}}_i}),\xi ))})}^2}}
$$&lt;/p&gt;

&lt;p&gt;where ${\bf \xi}$ is a $6$-vector representing the pose of the current image $I$ with respect to the reference image $I_K$ in Lie algebra $\mathfrak{se}(3)$,
and $\omega$ is the 3D projective warp function that maps the pixel location ${\bf{p}}_i$ in the reference image according to its inverse depth $D_K ({\bf{p}}_i)$
and the pose ${\bf{\xi}}$ to the pixel location in the current image.&lt;/p&gt;

&lt;h3 id=&#34;qulitative-3d-reconstruction-demonstration-comparisson-to-the-single-channel-grascale-method-video&#34;&gt;Qulitative 3D reconstruction demonstration/comparisson to the single-channel (grascale) method video:&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/WA55baA23Zs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>GUMS: A Generalized Unified Model for Stereo Omnidirectional Vision (Demonstrated Via a Folded Catadioptric System)</title>
      <link>https://ubuntuslave.github.io/publication/2016-iros-gums/</link>
      <pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2016-iros-gums/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;gums-coaxially_constrained-white_bg.png&#34; alt=&#34;Coaxially constrained GUMS&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;calibration-demonstration-video&#34;&gt;Calibration demonstration video:&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8c7fTHMSUFM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor Micro Aerial Vehicles (MAVs)</title>
      <link>https://ubuntuslave.github.io/publication/2016-sensors-omnistereo_sensor/</link>
      <pubDate>Sat, 06 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2016-sensors-omnistereo_sensor/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;graphical_representation_of_the_paper-white_bg.png&#34; alt=&#34;Graphical representation of the article&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Quadrotor Flight Using Onboard RGB-D Visual Odometry</title>
      <link>https://ubuntuslave.github.io/publication/2014-icra-uav/</link>
      <pubDate>Sat, 31 May 2014 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2014-icra-uav/</guid>
      <description>&lt;p&gt;Four-dimensional path (blue) in a cluttered indoor environment (map) built online with the visual odometry algorithm using the RGB-D sensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ICRA2014-path_goals.png&#34; alt=&#34;Path for the quadrotor in the clutter map&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A 6-degree-of-freedom (6-DoF) pose localization method for a monocular camera in a 3D point-cloud dense map prebuilt by an RGB-D sensor.</title>
      <link>https://ubuntuslave.github.io/publication/2013-robio-6dof/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2013-robio-6dof/</guid>
      <description>

&lt;h2 id=&#34;6-dof-pose-localization-algorithm&#34;&gt;6-DoF Pose Localization Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;ROBIO2013-digest.png&#34; alt=&#34;Localization Pipeline&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The virtual view is constructed by projecting the map&amp;rsquo;s 3D points to a plane using the $t-1$ pose.&lt;/li&gt;
&lt;li&gt;2D features are matched between the real and virtual images.&lt;/li&gt;
&lt;li&gt;2D-to-3D point correspondences are obtained between the real camera&amp;rsquo;s 2D features and associated 3D points in the map.&lt;/li&gt;
&lt;li&gt;After Perspective-n-Point (PnP) + RANSAC, the relative 6-DoF transformation between the real and virtual cameras is found.&lt;/li&gt;
&lt;li&gt;A final frame transformation localizes the 6-DoF pose of the camera with respect to the map.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A Single-Camera Omni-Stereo Vision System for 3D Perception of Micro Aerial Vehicles (MAVs)</title>
      <link>https://ubuntuslave.github.io/publication/2013-iciea-omnistereo/</link>
      <pubDate>Wed, 19 Jun 2013 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2013-iciea-omnistereo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Incremental Registration of RGB-D Images</title>
      <link>https://ubuntuslave.github.io/publication/2012-icra-incremental/</link>
      <pubDate>Mon, 14 May 2012 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2012-icra-incremental/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating near-spherical range panoramas by fusing optical flow and stereo from a single-camera folded catadioptric rig</title>
      <link>https://ubuntuslave.github.io/publication/2011-mvap-spherical_omnistereo/</link>
      <pubDate>Sat, 17 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2011-mvap-spherical_omnistereo/</guid>
      <description>

&lt;h3 id=&#34;geometric-model&#34;&gt;Geometric Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;spherical_rig-geometric_model.png&#34; alt=&#34;Spherical omnistereo geometric model&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;non-single-viewpoint-non-svp-constraint&#34;&gt;Non Single Viewpoint (non-SVP) Constraint&lt;/h3&gt;

&lt;p&gt;We set virtual camera $F$&amp;rsquo;  at cusp of caustic $C$ of viewpoints. The following animation exemplifies the respective projection $u$ of a point $P$ and the change in uncertainty ${\delta \varphi}$ due to the incidence angle (elevation angle ${\varphi}$).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;non-SVP_caustic-animation.gif&#34; alt=&#34;Non-SVP caustic due to projection angles&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fusing Optical Flow and Stereo in a Spherical Depth Panorama Using a Single-Camera Folded Catadioptric Rig</title>
      <link>https://ubuntuslave.github.io/publication/2011-icra-fusing_optical_flow/</link>
      <pubDate>Mon, 09 May 2011 00:00:00 +0000</pubDate>
      <guid>https://ubuntuslave.github.io/publication/2011-icra-fusing_optical_flow/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;best_CV_paper-finalist-certificate.png&#34; alt=&#34;Best Vision Paper Award - Finalist&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
