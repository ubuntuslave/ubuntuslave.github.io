[{"authors":["carlos"],"categories":null,"content":"I am currently working as a Senior Autonomy Engineer at Sea Machines, where I am developing practical path-planning algorithms to assist the autonomous navigation of manned ships (e.g., the SM-300), and target detection \u0026amp; classification with monocular computer vision (AI-ris).\n I worked as a Senior Robotics Engineer at Piaggio Fast Forward (PFF), where we were building the future of personal mobile carrier robots, gita.\nPreviously, I was a Perception Engineer at Aurora Flight Sciences, a Boeing Company working on aerospace autonomy. In 2018, I earned my doctorate degree in computer science at the City University of New York under the supervision of Dr. Jizhong Xiao at the CCNY Robotics Lab. My Ph.D thesis dissertation included research in topics of computer vision applied to robotic sensing for navigation, mobile autonomous robots and omnidirectional vision sensors. I enjoy researching science, hacking technology, and programming in any language that gets the job done, but my preferences are Python, C and modern C++. I have participated in various international events, conferences, and competitions. For instance, in 2009, I was part of the CityALIEN team, which won the Design Competition during the Intelligent Ground Vehicle Competition (IGVC), and then I led a new team in 2011. I interned at Mitsubishi Electric Research Laboratories (MERL) for a year (2016-2017) where I was able to collaborate with Dr. Yuichi Taguchi and Dr. Chen Feng on developing algorithms for SLAM (simultaneous localization and mapping) and 3D reconstruction.\n","date":1682294400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1682294400,"objectID":"8d0972dd4ff800cd6d19f264079261ef","permalink":"https://ubuntuslave.github.io/authors/carlos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carlos/","section":"authors","summary":"I am currently working as a Senior Autonomy Engineer at Sea Machines, where I am developing practical path-planning algorithms to assist the autonomous navigation of manned ships (e.g., the SM-300), and target detection \u0026amp; classification with monocular computer vision (AI-ris).\n I worked as a Senior Robotics Engineer at Piaggio Fast Forward (PFF), where we were building the future of personal mobile carrier robots, gita.\nPreviously, I was a Perception Engineer at Aurora Flight Sciences, a Boeing Company working on aerospace autonomy.","tags":null,"title":"Carlos Jaramillo","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://ubuntuslave.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Carlos Jaramillo"],"categories":["Interview"],"content":" ","date":1682294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682294400,"objectID":"5f65b87bf04e8037a44d169973e82c6f","permalink":"https://ubuntuslave.github.io/post/smr-acmm/","publishdate":"2023-04-24T00:00:00Z","relpermalink":"/post/smr-acmm/","section":"post","summary":"Carlos, SMR's Senior Autonomy Engineer, is here to tell us what is, in his opinion, the most surprising way that autonomy is making the maritime sector kinder to our planet.","tags":["Professional","Maritime"],"title":"Ask a Crew Member Monday","type":"post"},{"authors":null,"categories":null,"content":" Demonstration video using sequences from Grand Central Terminal (GCT):   Source code The demonstration code can be found at the git repository for vo_single_camera_sos\nCalibration Files Data sets Each sequence has 2 sub-folders:\n omni pertaining the omnistereo data rgbd pertaining the RGB-D camera data   Remarks about the \u0026ldquo;ground-truth\u0026rdquo; data used as reference  Ground-truth poses obey the TUM format, such that each line is spaced-separated encoding:\n time stamp $t_x$ $t_y$ $t_z$ $q_x$ $q_y$ $q_z$ $q_w$  For each sequence, the gt_TUM.txt is the raw ground-truth data, which was obtained from the motion capture system. Thus, these poses are given wrt our VICON mocap\u0026rsquo;s frame, $[\\rm{V}]$.\n After running the demo_vo_*.py for some sequence, the resulting files will be given inside the results subfolder within the sequence path:\n estimated_frame_poses_TUM.txt has the estimated poses ${}_{[{{\\rm{C}}_i}]}^{[{\\rm{K}_0}]}{\\bf{\\tilde T}}$ of the sequence wrt the initial camera frame, $[\\rm{K}_0]$. gt_associated_frame_poses_TUM.txt has the associated ground-truth poses for the registered frames. They are already transformed into the camera frame, $[\\textbf{C}]$, via the appropriate hand-eye transformation, so that the pose is given as ${}_{[{{\\rm{C}}_i}]}^{[{\\rm{K}_0}]}{\\bf{T}}$.   For the real-life sequences of the RGB-D camera, the required hand-eye transformation can be downloaded from this link rgbd_hand_eye_transformation.txt.\n  Synthetic sequences To run the demo_vo_sos.py script with the synthetic data set, it suffices to obtain the corresponding GUMS calibration file gums-calibrated.pkl\n  Name # Frames Video Sample   Office-0 1508     Office-1 965   Office-2 880   Office-3 1240   Real-life sequences  To run the demo_vo_sos.py script with the real-life SOS data set, it suffices to obtain the corresponding GUMS calibration file gums-calibrated.pkl\n To run the demo_vo_rgbd.py script with the real-life RGB-D data set, the required hand-eye transformation rgbd_hand_eye_transformation.txt is needed.\n  Conventional motion    Name # Frames     Square Small 619   Square Smooth 1325   Spinning 770   Vertical 459   Free Style 611   Hallway 5636    Moving under special conditions    Name # Frames     Into Wall - Regular 1041   Into Wall - Slow 1400   Into Wall - Fast 896   Into Wall - Curvy 838   Into Dark - Straight 998   Into Dark - Turning 1260    Moving in dynamic environments    Name # Frames     Slow Dynamic 390   Fast Dynamic 518   GCT Clock 2179   GCT Stairs 3625    Static rigs in dynamic environments    Prox. [m] # People File Link # Frames     1 1 static_dynamic_1_1.zip 691   1 2 static_dynamic_1_2.zip 759   1 4 static_dynamic_1_4.zip 791   2 1 static_dynamic_2_1.zip 679   2 2 static_dynamic_2_2.zip 673   2 4 static_dynamic_2_4.zip 799   3 1 static_dynamic_3_1.zip 720   3 2 static_dynamic_3_2.zip 815   3 4 static_dynamic_3_4.zip 772   Var 2 static_dynamic_freestyle.zip 939   Var Var GCT_static.zip 1904    Citation When using this dataset in your research, please cite:\n @ARTICLE{Jaramillo2019MVAP, \u0026nbsp; author = {Carlos Jaramillo and Liang Yang and Pablo Munoz and Yuichi Taguchi and Jizhong Xiao },  \u0026nbsp; title = {Visual Odometry with a Single-Camera Stereo Omnidirectional System},\u0026nbsp; journal = {Springer Machine Vision and Applications (MVAP)}, \u0026nbsp; year = {2019} }   Copyright  All datasets on this page are copyrighted by Carlos Jaramillo and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License.  This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.\n ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"21f9630b9a071c352514c136e08810b8","permalink":"https://ubuntuslave.github.io/project/vo_sos/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/vo_sos/","section":"project","summary":"A single-camera stereo omnidirectional system (SOS) is applied for estimating egomotion in real-world environments.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV","Visual Odometry","Camera Tracking","Pose Estimation"],"title":"Visual Odometry with a Single-Camera Stereo Omnidirectional System","type":"project"},{"authors":["Carlos Jaramillo","Liang Yang","J. Pablo Munoz","Yuichi Taguchi","Jizhong Xiao"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7c18b488cfc0cf4ede9cd7afee41d586","permalink":"https://ubuntuslave.github.io/publication/2019-mvap-sos_vo/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/2019-mvap-sos_vo/","section":"publication","summary":"We present the advantages of a single-camera stereo omnidirectional system (SOS) in estimating egomotion in real-world environments.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Visual Odometry","Camera Tracking"],"title":"Visual Odometry with a Single-Camera Stereo Omnidirectional System","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ubuntuslave.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Carlos Jaramillo"],"categories":null,"content":"","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"2db806fbd9a83a57d55ef979fef9b094","permalink":"https://ubuntuslave.github.io/publication/2018-phd_thesis/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/publication/2018-phd_thesis/","section":"publication","summary":"We explore low-cost solutions for efficiently improving the 3D pose estimation problem of a single omnidirectional camera moving in an unfamiliar environment.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Omnistereo","Catadioptrics","Visual Odometry","Pose Estimation","Camera Tracking","Calibration"],"title":"Enhancing 3D Visual Odometry with Single-Camera Stereo Omnidirectional Systems","type":"publication"},{"authors":["Carlos Jaramillo","Yuichi Taguchi","Chen Feng"],"categories":null,"content":" Theory The following figure explains the pipeline of the DMT system:\nThe single channel energy equation for photometric alignment is $$ E(\\xi ) = \\sum\\limits_{i = 1}^V {{{({{I_K}({{\\bf{p}}_i}) - I(\\omega ({{\\bf{p}}_i},{{\\bf{D}}_K}({{\\bf{p}}_i}),\\xi ))})}^2}} $$\nwhere ${\\bf \\xi}$ is a $6$-vector representing the pose of the current image $I$ with respect to the reference image $I_K$ in Lie algebra $\\mathfrak{se}(3)$, and $\\omega$ is the 3D projective warp function that maps the pixel location ${\\bf{p}}_i$ in the reference image according to its inverse depth $D_K ({\\bf{p}}_i)$ and the pose ${\\bf{\\xi}}$ to the pixel location in the current image.\nQulitative 3D reconstruction demonstration/comparisson to the single-channel (grascale) method video:   ","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495670400,"objectID":"39c3abb0a14ddcbe09105581858626ba","permalink":"https://ubuntuslave.github.io/publication/2017-3dv-dmt/","publishdate":"2017-05-25T00:00:00Z","relpermalink":"/publication/2017-3dv-dmt/","section":"publication","summary":"We present direct multichannel tracking, an algorithm for tracking the pose of a monocular camera (visual odometry) using high-dimensional features in a direct image alignment framework.","tags":["Computer Vision","Visual Odometry","SLAM","Pose Estimation","Camera Tracking"],"title":"Direct Multichannel Tracking","type":"publication"},{"authors":["Carlos Jaramillo","Roberto G. Valenti","Jizhong Xiao"],"categories":null,"content":" Calibration demonstration video:   ","date":1475971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475971200,"objectID":"b03906cdf8efbed8ad4b90e764ea4159","permalink":"https://ubuntuslave.github.io/publication/2016-iros-gums/","publishdate":"2016-10-09T00:00:00Z","relpermalink":"/publication/2016-iros-gums/","section":"publication","summary":"GUMS is a complete projection model for omnidirectional stereo vision systems. GUMS is based on the existing generalized unified model (GUM), which we extend for fixed baseline sensors.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Calibration"],"title":"GUMS: A Generalized Unified Model for Stereo Omnidirectional Vision (Demonstrated Via a Folded Catadioptric System)","type":"publication"},{"authors":["Carlos Jaramillo","Roberto G. Valenti","Ling Guo","Jizhong Xiao"],"categories":null,"content":"","date":1454716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454716800,"objectID":"6daf092139c47e2cc0653451a68eadc4","permalink":"https://ubuntuslave.github.io/publication/2016-sensors-omnistereo_sensor/","publishdate":"2016-02-06T00:00:00Z","relpermalink":"/publication/2016-sensors-omnistereo_sensor/","section":"publication","summary":"We describe the design and 3D sensing performance of an omnidirectional stereo (omnistereo) vision system applied to Micro Aerial Vehicles (MAVs).","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV"],"title":"Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor Micro Aerial Vehicles (MAVs)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1419292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419292800,"objectID":"20215db69e381605e9cadb1739f2f834","permalink":"https://ubuntuslave.github.io/project/ml-classification_from_luts/","publishdate":"2014-12-23T00:00:00Z","relpermalink":"/project/ml-classification_from_luts/","section":"project","summary":"Project for Prof. Robert Haralick's Machine Learning Course at CUNY Graduate Center. The goal was to design a labeled two class data set of 10-dimensional vectors that has test set classification accuracy less than 60% on some popular classifiers. However, our decision rule was designed such that it can perform with greater than 90% accuracy on the test set.","tags":["Machine Learning"],"title":"Machine Learning Project - Multidimensional Classification using LUTs","type":"project"},{"authors":["Roberto G. Valenti","Ivan Dryanovski","Carlos Jaramillo","Daniel Perea Strom","Jizhong Xiao"],"categories":null,"content":"Four-dimensional path (blue) in a cluttered indoor environment (map) built online with the visual odometry algorithm using the RGB-D sensor.\n","date":1401494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401494400,"objectID":"efb07aabd4d12728195f183bf1fd20c7","permalink":"https://ubuntuslave.github.io/publication/2014-icra-uav/","publishdate":"2014-05-31T00:00:00Z","relpermalink":"/publication/2014-icra-uav/","section":"publication","summary":"We present an on-board navigation system for Micro Aerial Vehicles (MAV) based on information provided by a visual odometry algorithm processing data from an RGB-D camera.","tags":["Computer Vision","Visual Odometry","SLAM","Pose Estimation","Small UAVs","Robotics","RGB-D"],"title":"Autonomous Quadrotor Flight Using Onboard RGB-D Visual Odometry","type":"publication"},{"authors":["Carlos Jaramillo","Ivan Dryanovski","Roberto G. Valenti","Jizhong Xiao"],"categories":null,"content":" 6-DoF Pose Localization Algorithm  The virtual view is constructed by projecting the map\u0026rsquo;s 3D points to a plane using the $t-1$ pose. 2D features are matched between the real and virtual images. 2D-to-3D point correspondences are obtained between the real camera\u0026rsquo;s 2D features and associated 3D points in the map. After Perspective-n-Point (PnP) + RANSAC, the relative 6-DoF transformation between the real and virtual cameras is found. A final frame transformation localizes the 6-DoF pose of the camera with respect to the map.  ","date":1386806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1386806400,"objectID":"915c1493e1bac0339690eede80a26b2a","permalink":"https://ubuntuslave.github.io/publication/2013-robio-6dof/","publishdate":"2013-12-12T00:00:00Z","relpermalink":"/publication/2013-robio-6dof/","section":"publication","summary":"A 6-degree-of-freedom (6-DoF) pose localization method for a monocular camera in a 3D point-cloud dense map prebuilt by an RGB-D sensor.","tags":["Computer Vision","Localization","Pose Estimation","Camera Tracking","RGB-D"],"title":"A 6-degree-of-freedom (6-DoF) pose localization method for a monocular camera in a 3D point-cloud dense map prebuilt by an RGB-D sensor.","type":"publication"},{"authors":["Carlos Jaramillo","Ling Guo","Jizhong Xiao"],"categories":null,"content":"","date":1371600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1371600000,"objectID":"fdda2d45430526b4a6afab36383b757a","permalink":"https://ubuntuslave.github.io/publication/2013-iciea-omnistereo/","publishdate":"2013-06-19T00:00:00Z","relpermalink":"/publication/2013-iciea-omnistereo/","section":"publication","summary":"We introduce a catadioptric single-camera omnistereo vision system that uses a pair of custom-designed mirrors (in a folded configuration) satisfying the single view point (SVP) property as a good solution to the perception challenge of MAVs.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV"],"title":"A Single-Camera Omni-Stereo Vision System for 3D Perception of Micro Aerial Vehicles (MAVs)","type":"publication"},{"authors":["Igor Labutov","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":" Geometric Model Non Single Viewpoint (non-SVP) Constraint We set virtual camera $F$\u0026rsquo; at cusp of caustic $C$ of viewpoints. The following animation exemplifies the respective projection $u$ of a point $P$ and the change in uncertainty ${\\delta \\varphi}$ due to the incidence angle (elevation angle ${\\varphi}$).\n","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"8f24af4a98bdc66315d5144f9622273a","permalink":"https://ubuntuslave.github.io/publication/2013-mvap-spherical_omnistereo/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/2013-mvap-spherical_omnistereo/","section":"publication","summary":"We design a novel 'folded' spherical catadioptric rig (formed by two coaxially-aligned spherical mirrors of distinct radii and a single perspective camera) to recover near-spherical range panoramas.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Catadioptrics"],"title":"Generating near-spherical range panoramas by fusing optical flow and stereo from a single-camera folded catadioptric rig","type":"publication"},{"authors":["Ivan Dryanovski","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":"","date":1336953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336953600,"objectID":"2ff0ff8aa813e4bea7657c366e4859d7","permalink":"https://ubuntuslave.github.io/publication/2012-icra-incremental/","publishdate":"2012-05-14T00:00:00Z","relpermalink":"/publication/2012-icra-incremental/","section":"publication","summary":"We present a real-time technique for 6-DoF camera pose estimation through the incremental registration of RGB-D images.","tags":["Computer Vision","Visual Odometry","Pose Estimation","Robotics","RGB-D"],"title":"Incremental Registration of RGB-D Images","type":"publication"},{"authors":null,"categories":null,"content":"In 2011, we engineered an autonomous vehicle with a simplified electrical architecture (focusing in safety and usability) and by adopting a new software architecture based on the open-source Robotics Operating System (ROS) framework, which enforces modularity and guarantees maintainability and reusability.\nDownload our Design Report\n","date":1307145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1307145600,"objectID":"808a7d20440658abe6ac0e8b3274986b","permalink":"https://ubuntuslave.github.io/project/cata/","publishdate":"2011-06-04T00:00:00Z","relpermalink":"/project/cata/","section":"project","summary":"In 2011, a new intelligent ground vehicle CATA (City Autonomous Transportation Agent) was rebuilt to employ the ROS framework to participate in IGVC.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Robotics","Ground Vehicle","Competition","Autonomous Navigation"],"title":"CATA","type":"project"},{"authors":["Igor Labutov","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":"","date":1304899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304899200,"objectID":"66221c20c284ebcc56a0e84e63889133","permalink":"https://ubuntuslave.github.io/publication/2011-icra-fusing_optical_flow/","publishdate":"2011-05-09T00:00:00Z","relpermalink":"/publication/2011-icra-fusing_optical_flow/","section":"publication","summary":"We design a novel 'folded' spherical catadioptric rig (formed by two coaxially-aligned spherical mirrors of distinct radii and a single perspective camera) to recover near-spherical range panoramas.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Catadioptrics"],"title":"Fusing Optical Flow and Stereo in a Spherical Depth Panorama Using a Single-Camera Folded Catadioptric Rig","type":"publication"},{"authors":null,"categories":null,"content":"During 2009-2010, I participated in the design of the City College\u0026rsquo;s IGVC 2010 rover (CityALIEN) by incorporating a novel approach based on stereo and omnidirectional vision. Our design won the First Place in the Design Competition.\nDownload our Design Report\n  ","date":1275609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1275609600,"objectID":"924f0c787004a1bf8cd16631b8ae8590","permalink":"https://ubuntuslave.github.io/project/cityalien/","publishdate":"2010-06-04T00:00:00Z","relpermalink":"/project/cityalien/","section":"project","summary":"In 2010, our intelligent ground vehicle CityALIEN participated and won the IGVC Design Challenge.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Robotics","Ground Vehicle","Competition","Autonomous Navigation"],"title":"CityALIEN","type":"project"},{"authors":null,"categories":null,"content":" In 2009, as part of my Research Experience for Undergraduates, I developed software for different types of small, educational robots, such as the Mindstorms Robotics Invention System, IPRE Scribbler, and Surveyor SRV-1\nSource Code:  Player/Stage Driver for the Surveyor Player/Stage control GUI for the Surveyor (SRVjoy) or any Player-based mobile robot  This is an example of the simulated SRV-1 robot on the occupancy grid (on Stage): ","date":1251763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1251763200,"objectID":"a9fc79b39e85c2dec89eb34089c369d9","permalink":"https://ubuntuslave.github.io/project/surveyor/","publishdate":"2009-09-01T00:00:00Z","relpermalink":"/project/surveyor/","section":"project","summary":"As part of my Research Experience for Undergraduates, I developed a Player/Stage driver and remote controller for the mobile robot.","tags":["Robotics","Ground Vehicle"],"title":"Surveyor SRV-1 Driver and GUI Remote Control","type":"project"}]