[{"authors":["carlos"],"categories":null,"content":"I am currently working as a Robotics Engineer at Piaggio Fast Forward (PFF), where we are building the future of personal mobile robots (gita). Prior to joining PFF, I was a Perception Engineer at Aurora Flight Sciences, a Boeing Company working on aerospace autonomy. In 2018, I earned my doctorate degree in computer science at the City University of New York under the supervision of Dr. Jizhong Xiao at the CCNY Robotics Lab. My Ph.D thesis dissertation included research in topics of computer vision applied to robotic sensing for navigation, mobile autonomous robots and omnidirectional vision sensors. I enjoy researching science, hacking technology, and programming in any language that gets the job done, but my preferences are Python, C and modern C++. I have participated in various international events, conferences, and competitions. For instance, in 2009, I was part of the CityALIEN team, which won the Design Competition during the Intelligent Ground Vehicle Competition (IGVC), and then I led a new team in 2011. I interned at Mitsubishi Electric Research Laboratories (MERL) for a year (2016-2017) where I was able to collaborate with Dr. Yuichi Taguchi and Dr. Chen Feng on developing algorithms for SLAM (simultaneous localization and mapping) and 3D reconstruction.\n","date":1569888000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1569888000,"objectID":"8d0972dd4ff800cd6d19f264079261ef","permalink":"https://ubuntuslave.github.io/authors/carlos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carlos/","section":"authors","summary":"I am currently working as a Robotics Engineer at Piaggio Fast Forward (PFF), where we are building the future of personal mobile robots (gita). Prior to joining PFF, I was a Perception Engineer at Aurora Flight Sciences, a Boeing Company working on aerospace autonomy. In 2018, I earned my doctorate degree in computer science at the City University of New York under the supervision of Dr. Jizhong Xiao at the CCNY Robotics Lab.","tags":null,"title":"Carlos Jaramillo","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://ubuntuslave.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Demonstration video using sequences from Grand Central Terminal (GCT):   Source code The demonstration code can be found at the git repository for vo_single_camera_sos\nCalibration Files Data sets Each sequence has 2 sub-folders:\n omni pertaining the omnistereo data rgbd pertaining the RGB-D camera data   Remarks about the \u0026ldquo;ground-truth\u0026rdquo; data used as reference  Ground-truth poses obey the TUM format, such that each line is spaced-separated encoding:\n time stamp $t_x$ $t_y$ $t_z$ $q_x$ $q_y$ $q_z$ $q_w$  For each sequence, the gt_TUM.txt is the raw ground-truth data, which was obtained from the motion capture system. Thus, these poses are given wrt our VICON mocap\u0026rsquo;s frame, $[\\rm{V}]$.\n After running the demo_vo_*.py for some sequence, the resulting files will be given inside the results subfolder within the sequence path:\n estimated_frame_poses_TUM.txt has the estimated poses ${}_{[{{\\rm{C}}_i}]}^{[{\\rm{K}_0}]}{\\bf{\\tilde T}}$ of the sequence wrt the initial camera frame, $[\\rm{K}_0]$. gt_associated_frame_poses_TUM.txt has the associated ground-truth poses for the registered frames. They are already transformed into the camera frame, $[\\textbf{C}]$, via the appropriate hand-eye transformation, so that the pose is given as ${}_{[{{\\rm{C}}_i}]}^{[{\\rm{K}_0}]}{\\bf{T}}$.   For the real-life sequences of the RGB-D camera, the required hand-eye transformation can be downloaded from this link rgbd_hand_eye_transformation.txt.\n  Synthetic sequences To run the demo_vo_sos.py script with the synthetic data set, it suffices to obtain the corresponding GUMS calibration file gums-calibrated.pkl\n  Name # Frames Video Sample   Office-0 1508     Office-1 965   Office-2 880   Office-3 1240   Real-life sequences  To run the demo_vo_sos.py script with the real-life SOS data set, it suffices to obtain the corresponding GUMS calibration file gums-calibrated.pkl\n To run the demo_vo_rgbd.py script with the real-life RGB-D data set, the required hand-eye transformation rgbd_hand_eye_transformation.txt is needed.\n  Conventional motion    Name # Frames     Square Small 619   Square Smooth 1325   Spinning 770   Vertical 459   Free Style 611   Hallway 5636    Moving under special conditions    Name # Frames     Into Wall - Regular 1041   Into Wall - Slow 1400   Into Wall - Fast 896   Into Wall - Curvy 838   Into Dark - Straight 998   Into Dark - Turning 1260    Moving in dynamic environments    Name # Frames     Slow Dynamic 390   Fast Dynamic 518   GCT Clock 2179   GCT Stairs 3625    Static rigs in dynamic environments    Prox. [m] # People File Link # Frames     1 1 static_dynamic_1_1.zip 691   1 2 static_dynamic_1_2.zip 759   1 4 static_dynamic_1_4.zip 791   2 1 static_dynamic_2_1.zip 679   2 2 static_dynamic_2_2.zip 673   2 4 static_dynamic_2_4.zip 799   3 1 static_dynamic_3_1.zip 720   3 2 static_dynamic_3_2.zip 815   3 4 static_dynamic_3_4.zip 772   Var 2 static_dynamic_freestyle.zip 939   Var Var GCT_static.zip 1904    Citation When using this dataset in your research, please cite:\n @ARTICLE{Jaramillo2019MVAP, \u0026nbsp; author = {Carlos Jaramillo and Liang Yang and Pablo Munoz and Yuichi Taguchi and Jizhong Xiao },  \u0026nbsp; title = {Visual Odometry with a Single-Camera Stereo Omnidirectional System},\u0026nbsp; journal = {Springer Machine Vision and Applications (MVAP)}, \u0026nbsp; year = {2019} }   Copyright  All datasets on this page are copyrighted by Carlos Jaramillo and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License.  This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.\n ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"21f9630b9a071c352514c136e08810b8","permalink":"https://ubuntuslave.github.io/project/vo_sos/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/vo_sos/","section":"project","summary":"A single-camera stereo omnidirectional system (SOS) is applied for estimating egomotion in real-world environments.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV","Visual Odometry","Camera Tracking","Pose Estimation"],"title":"Visual Odometry with a Single-Camera Stereo Omnidirectional System","type":"project"},{"authors":["Carlos Jaramillo","Liang Yang","J. Pablo Munoz","Yuichi Taguchi","Jizhong Xiao"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"7c18b488cfc0cf4ede9cd7afee41d586","permalink":"https://ubuntuslave.github.io/publication/2019-mvap-sos_vo/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/2019-mvap-sos_vo/","section":"publication","summary":"We present the advantages of a single-camera stereo omnidirectional system (SOS) in estimating egomotion in real-world environments.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Visual Odometry","Camera Tracking"],"title":"Visual Odometry with a Single-Camera Stereo Omnidirectional System","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file and adding markup: mmark to your page front matter.\nTo render inline or block math, wrap your LaTeX math with $$...$$.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n\\[\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}\\]\nExample inline math $$\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2$$ renders as \\(\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2\\) .\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n\\[f(k;p_0^*) = \\begin{cases} p_0^* \u0026 \\text{if }k=1, \\\\ 1-p_0^* \u0026 \\text {if }k=0.\\end{cases}\\]\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ```  renders as\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D;  An example sequence diagram:\n```mermaid sequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram participant Alice participant Bob Alice-\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail... John--\u0026gt;Alice: Great! John-\u0026gt;Bob: How about you? Bob--\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d ```  renders as\ngantt dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a Markdown extension for asides, also referred to as notices or hints. By prefixing a paragraph with A\u0026gt;, it will render as an aside. You can enable this feature by adding markup: mmark to your page front matter, or alternatively using the Alert shortcode.\nA\u0026gt; A Markdown aside is useful for displaying notices, hints, or definitions to your readers.  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.\n Did you find this page helpful? Consider sharing it üôå ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://ubuntuslave.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you'll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["admin"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://ubuntuslave.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ubuntuslave.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Carlos Jaramillo"],"categories":null,"content":"","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"2db806fbd9a83a57d55ef979fef9b094","permalink":"https://ubuntuslave.github.io/publication/2018-phd_thesis/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/publication/2018-phd_thesis/","section":"publication","summary":"We explore low-cost solutions for efficiently improving the 3D pose estimation problem of a single omnidirectional camera moving in an unfamiliar environment.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Omnistereo","Catadioptrics","Visual Odometry","Pose Estimation","Camera Tracking","Calibration"],"title":"Enhancing 3D Visual Odometry with Single-Camera Stereo Omnidirectional Systems","type":"publication"},{"authors":["Carlos Jaramillo","Yuichi Taguchi","Chen Feng"],"categories":null,"content":" Theory The following figure explains the pipeline of the DMT system:\nThe single channel energy equation for photometric alignment is $$ E(\\xi ) = \\sum\\limits_{i = 1}^V {{{({{I_K}({{\\bf{p}}_i}) - I(\\omega ({{\\bf{p}}_i},{{\\bf{D}}_K}({{\\bf{p}}_i}),\\xi ))})}^2}} $$\nwhere ${\\bf \\xi}$ is a $6$-vector representing the pose of the current image $I$ with respect to the reference image $I_K$ in Lie algebra $\\mathfrak{se}(3)$, and $\\omega$ is the 3D projective warp function that maps the pixel location ${\\bf{p}}_i$ in the reference image according to its inverse depth $D_K ({\\bf{p}}_i)$ and the pose ${\\bf{\\xi}}$ to the pixel location in the current image.\nQulitative 3D reconstruction demonstration/comparisson to the single-channel (grascale) method video:   ","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495670400,"objectID":"39c3abb0a14ddcbe09105581858626ba","permalink":"https://ubuntuslave.github.io/publication/2017-3dv-dmt/","publishdate":"2017-05-25T00:00:00Z","relpermalink":"/publication/2017-3dv-dmt/","section":"publication","summary":"We present direct multichannel tracking, an algorithm for tracking the pose of a monocular camera (visual odometry) using high-dimensional features in a direct image alignment framework.","tags":["Computer Vision","Visual Odometry","SLAM","Pose Estimation","Camera Tracking"],"title":"Direct Multichannel Tracking","type":"publication"},{"authors":["Carlos Jaramillo","Roberto G. Valenti","Jizhong Xiao"],"categories":null,"content":" Calibration demonstration video:   ","date":1475971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475971200,"objectID":"b03906cdf8efbed8ad4b90e764ea4159","permalink":"https://ubuntuslave.github.io/publication/2016-iros-gums/","publishdate":"2016-10-09T00:00:00Z","relpermalink":"/publication/2016-iros-gums/","section":"publication","summary":"GUMS is a complete projection model for omnidirectional stereo vision systems. GUMS is based on the existing generalized unified model (GUM), which we extend for fixed baseline sensors.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Calibration"],"title":"GUMS: A Generalized Unified Model for Stereo Omnidirectional Vision (Demonstrated Via a Folded Catadioptric System)","type":"publication"},{"authors":["admin"],"categories":["Demo"],"content":" Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n üëâ Get Started üìö View the documentation üí¨ Ask a question on the forum üë• Chat with the community üê¶ Twitter: @source_themes @GeorgeCushen #MadeWithAcademic üí° Request a feature or report a bug ‚¨ÜÔ∏è Updating? View the Update Guide and Release Notes ‚ù§Ô∏è Support development of Academic:  ‚òïÔ∏è Donate a coffee üíµ Become a backer on Patreon üñºÔ∏è Decorate your laptop or journal with an Academic sticker üëï Wear the T-shirt üë©‚Äçüíª Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://ubuntuslave.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Carlos Jaramillo","Roberto G. Valenti","Ling Guo","Jizhong Xiao"],"categories":null,"content":"","date":1454716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454716800,"objectID":"6daf092139c47e2cc0653451a68eadc4","permalink":"https://ubuntuslave.github.io/publication/2016-sensors-omnistereo_sensor/","publishdate":"2016-02-06T00:00:00Z","relpermalink":"/publication/2016-sensors-omnistereo_sensor/","section":"publication","summary":"We describe the design and 3D sensing performance of an omnidirectional stereo (omnistereo) vision system applied to Micro Aerial Vehicles (MAVs).","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV"],"title":"Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor Micro Aerial Vehicles (MAVs)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1419292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419292800,"objectID":"20215db69e381605e9cadb1739f2f834","permalink":"https://ubuntuslave.github.io/project/ml-classification_from_luts/","publishdate":"2014-12-23T00:00:00Z","relpermalink":"/project/ml-classification_from_luts/","section":"project","summary":"Project for Prof. Robert Haralick's Machine Learning Course at CUNY Graduate Center. The goal was to design a labeled two class data set of 10-dimensional vectors that has test set classification accuracy less than 60% on some popular classifiers. However, our decision rule was designed such that it can perform with greater than 90% accuracy on the test set.","tags":["Machine Learning"],"title":"Machine Learning Project - Multidimensional Classification using LUTs","type":"project"},{"authors":["Roberto G. Valenti","Ivan Dryanovski","Carlos Jaramillo","Daniel Perea Strom","Jizhong Xiao"],"categories":null,"content":"Four-dimensional path (blue) in a cluttered indoor environment (map) built online with the visual odometry algorithm using the RGB-D sensor.\n","date":1401494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401494400,"objectID":"efb07aabd4d12728195f183bf1fd20c7","permalink":"https://ubuntuslave.github.io/publication/2014-icra-uav/","publishdate":"2014-05-31T00:00:00Z","relpermalink":"/publication/2014-icra-uav/","section":"publication","summary":"We present an on-board navigation system for Micro Aerial Vehicles (MAV) based on information provided by a visual odometry algorithm processing data from an RGB-D camera.","tags":["Computer Vision","Visual Odometry","SLAM","Pose Estimation","Small UAVs","Robotics","RGB-D"],"title":"Autonomous Quadrotor Flight Using Onboard RGB-D Visual Odometry","type":"publication"},{"authors":["Carlos Jaramillo","Ivan Dryanovski","Roberto G. Valenti","Jizhong Xiao"],"categories":null,"content":" 6-DoF Pose Localization Algorithm  The virtual view is constructed by projecting the map\u0026rsquo;s 3D points to a plane using the $t-1$ pose. 2D features are matched between the real and virtual images. 2D-to-3D point correspondences are obtained between the real camera\u0026rsquo;s 2D features and associated 3D points in the map. After Perspective-n-Point (PnP) + RANSAC, the relative 6-DoF transformation between the real and virtual cameras is found. A final frame transformation localizes the 6-DoF pose of the camera with respect to the map.  ","date":1386806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1386806400,"objectID":"915c1493e1bac0339690eede80a26b2a","permalink":"https://ubuntuslave.github.io/publication/2013-robio-6dof/","publishdate":"2013-12-12T00:00:00Z","relpermalink":"/publication/2013-robio-6dof/","section":"publication","summary":"A 6-degree-of-freedom (6-DoF) pose localization method for a monocular camera in a 3D point-cloud dense map prebuilt by an RGB-D sensor.","tags":["Computer Vision","Localization","Pose Estimation","Camera Tracking","RGB-D"],"title":"A 6-degree-of-freedom (6-DoF) pose localization method for a monocular camera in a 3D point-cloud dense map prebuilt by an RGB-D sensor.","type":"publication"},{"authors":["Carlos Jaramillo","Ling Guo","Jizhong Xiao"],"categories":null,"content":"","date":1371600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1371600000,"objectID":"fdda2d45430526b4a6afab36383b757a","permalink":"https://ubuntuslave.github.io/publication/2013-iciea-omnistereo/","publishdate":"2013-06-19T00:00:00Z","relpermalink":"/publication/2013-iciea-omnistereo/","section":"publication","summary":"We introduce a catadioptric single-camera omnistereo vision system that uses a pair of custom-designed mirrors (in a folded configuration) satisfying the single view point (SVP) property as a good solution to the perception challenge of MAVs.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Small UAV"],"title":"A Single-Camera Omni-Stereo Vision System for 3D Perception of Micro Aerial Vehicles (MAVs)","type":"publication"},{"authors":["Igor Labutov","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":" Geometric Model Non Single Viewpoint (non-SVP) Constraint We set virtual camera $F$\u0026rsquo; at cusp of caustic $C$ of viewpoints. The following animation exemplifies the respective projection $u$ of a point $P$ and the change in uncertainty ${\\delta \\varphi}$ due to the incidence angle (elevation angle ${\\varphi}$).\n","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"8f24af4a98bdc66315d5144f9622273a","permalink":"https://ubuntuslave.github.io/publication/2013-mvap-spherical_omnistereo/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/2013-mvap-spherical_omnistereo/","section":"publication","summary":"We design a novel 'folded' spherical catadioptric rig (formed by two coaxially-aligned spherical mirrors of distinct radii and a single perspective camera) to recover near-spherical range panoramas.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Catadioptrics"],"title":"Generating near-spherical range panoramas by fusing optical flow and stereo from a single-camera folded catadioptric rig","type":"publication"},{"authors":["Ivan Dryanovski","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":"","date":1336953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336953600,"objectID":"2ff0ff8aa813e4bea7657c366e4859d7","permalink":"https://ubuntuslave.github.io/publication/2012-icra-incremental/","publishdate":"2012-05-14T00:00:00Z","relpermalink":"/publication/2012-icra-incremental/","section":"publication","summary":"We present a real-time technique for 6-DoF camera pose estimation through the incremental registration of RGB-D images.","tags":["Computer Vision","Visual Odometry","Pose Estimation","Robotics","RGB-D"],"title":"Incremental Registration of RGB-D Images","type":"publication"},{"authors":null,"categories":null,"content":"In 2011, we engineered an autonomous vehicle with a simplified electrical architecture (focusing in safety and usability) and by adopting a new software architecture based on the open-source Robotics Operating System (ROS) framework, which enforces modularity and guarantees maintainability and reusability.\nDownload our Design Report\n","date":1307145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1307145600,"objectID":"808a7d20440658abe6ac0e8b3274986b","permalink":"https://ubuntuslave.github.io/project/cata/","publishdate":"2011-06-04T00:00:00Z","relpermalink":"/project/cata/","section":"project","summary":"In 2011, a new intelligent ground vehicle CATA (City Autonomous Transportation Agent) was rebuilt to employ the ROS framework to participate in IGVC.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Robotics","Ground Vehicle","Competition","Autonomous Navigation"],"title":"CATA","type":"project"},{"authors":["Igor Labutov","Carlos Jaramillo","Jizhong Xiao"],"categories":null,"content":"","date":1304899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304899200,"objectID":"66221c20c284ebcc56a0e84e63889133","permalink":"https://ubuntuslave.github.io/publication/2011-icra-fusing_optical_flow/","publishdate":"2011-05-09T00:00:00Z","relpermalink":"/publication/2011-icra-fusing_optical_flow/","section":"publication","summary":"We design a novel 'folded' spherical catadioptric rig (formed by two coaxially-aligned spherical mirrors of distinct radii and a single perspective camera) to recover near-spherical range panoramas.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Stereo Vision","Catadioptrics"],"title":"Fusing Optical Flow and Stereo in a Spherical Depth Panorama Using a Single-Camera Folded Catadioptric Rig","type":"publication"},{"authors":null,"categories":null,"content":"During 2009-2010, I participated in the design of the City College\u0026rsquo;s IGVC 2010 rover (CityALIEN) by incorporating a novel approach based on stereo and omnidirectional vision. Our design won the First Place in the Design Competition.\nDownload our Design Report\n  ","date":1275609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1275609600,"objectID":"924f0c787004a1bf8cd16631b8ae8590","permalink":"https://ubuntuslave.github.io/project/cityalien/","publishdate":"2010-06-04T00:00:00Z","relpermalink":"/project/cityalien/","section":"project","summary":"In 2010, our intelligent ground vehicle CityALIEN participated and won the IGVC Design Challenge.","tags":["Computer Vision","Omnidirectional Vision","Panoramic Vision","Omnistereo","Stereo Vision","Catadioptrics","Robotics","Ground Vehicle","Competition","Autonomous Navigation"],"title":"CityALIEN","type":"project"},{"authors":null,"categories":null,"content":" In 2009, as part of my Research Experience for Undergraduates, I developed software for different types of small, educational robots, such as the Mindstorms Robotics Invention System, IPRE Scribbler, and Surveyor SRV-1\nSource Code:  Player/Stage Driver for the Surveyor Player/Stage control GUI for the Surveyor (SRVjoy) or any Player-based mobile robot  This is an example of the simulated SRV-1 robot on the occupancy grid (on Stage): ","date":1251763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1251763200,"objectID":"a9fc79b39e85c2dec89eb34089c369d9","permalink":"https://ubuntuslave.github.io/project/surveyor/","publishdate":"2009-09-01T00:00:00Z","relpermalink":"/project/surveyor/","section":"project","summary":"As part of my Research Experience for Undergraduates, I developed a Player/Stage driver and remote controller for the mobile robot.","tags":["Robotics","Ground Vehicle"],"title":"Surveyor SRV-1 Driver and GUI Remote Control","type":"project"}]